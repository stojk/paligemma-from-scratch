{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Please refer to \"vit(gemma)_from_head.ipynb\" notebooks for introdution to ViT and Gemma models*\n",
    "\n",
    "*In this notebook we will just combine the two*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from gemma import Gemma\n",
    "from vit import ViT\n",
    "\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load reference HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.27s/it]"
     ]
    }
   ],
   "source": [
    "model_id = \"google/paligemma-3b-mix-224\"\n",
    "pg_model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bee\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is on the flower?\"\n",
    "image_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "output = pg_model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original preprocessor tokenizes the prompt for Gemma\n",
    "# and preprocesses the image for ViT, and then combines the two in the dictionary\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_weights = pg_model.language_model.state_dict()\n",
    "video_weights = pg_model.vision_tower.vision_model.state_dict()\n",
    "multi_modal_projector_weights = pg_model.multi_modal_projector.state_dict()\n",
    "config = pg_model.config\n",
    "pg_text_config = config.text_config\n",
    "pg_vision_config = config.vision_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load custom Gemma and ViT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = Gemma(\n",
    "    dim = pg_text_config.hidden_size,\n",
    "    n_layers = pg_text_config.num_hidden_layers,\n",
    "    n_heads = pg_text_config.num_attention_heads,\n",
    "    num_key_value_heads = pg_text_config.num_key_value_heads,\n",
    "    fc_intermediate_size = pg_text_config.intermediate_size,\n",
    "    vocab_size = pg_text_config.vocab_size,\n",
    "    rms_norm_eps = pg_text_config.rms_norm_eps,\n",
    "    max_position_embeddings = pg_text_config.max_position_embeddings,\n",
    "    with_embedding = False,\n",
    ")\n",
    "gemma.load_hf_weights(text_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 257216])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.0916,   5.1240, -15.7540,  ...,  -0.8838,  -0.8866,  -0.8847],\n",
       "         [  0.6308,   4.3901, -20.1562,  ...,  -0.8476,  -0.8535,  -0.8455],\n",
       "         [ -0.1848,   5.5989, -21.0075,  ...,  -0.8385,  -0.8447,  -0.8407],\n",
       "         ...,\n",
       "         [ -0.1042,   0.3676, -10.0569,  ...,  -1.7666,  -1.7766,  -1.7793],\n",
       "         [  0.3203,   0.7445, -15.5378,  ...,  -1.6099,  -1.6231,  -1.6265],\n",
       "         [  1.3958,   5.7269, -15.0676,  ...,  -0.5363,  -0.5472,  -0.5411]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[  5706, 125942, 151223, 139977,  96629, 160977, 251909, 209214,   6190,\n",
    "         102413, 247227,  84615,  12321, 102069, 250598, 165257, 213011, 223305,\n",
    "         108701, 223214]])\n",
    "\n",
    "cutom_out = gemma(x)\n",
    "print(cutom_out.shape)\n",
    "cutom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    dim=pg_vision_config.hidden_size,\n",
    "    n_channels=pg_vision_config.num_channels,\n",
    "    n_layers=pg_vision_config.num_hidden_layers,\n",
    "    n_heads=pg_vision_config.num_attention_heads,\n",
    "    image_size=pg_vision_config.image_size,\n",
    "    patch_size=pg_vision_config.patch_size,\n",
    "    fc_intermediate_size=pg_vision_config.intermediate_size,\n",
    "    norm_eps=pg_vision_config.layer_norm_eps,\n",
    ")\n",
    "vit.load_hf_weights(video_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_to_text_embedding = torch.nn.Linear(\n",
    "    pg_vision_config.hidden_size,\n",
    "    pg_text_config.hidden_size,\n",
    "    bias=True\n",
    ")\n",
    "visual_embedding_to_text_embedding.weight.data = multi_modal_projector_weights['linear.weight']\n",
    "visual_embedding_to_text_embedding.bias.data = multi_modal_projector_weights['linear.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_layer = nn.Embedding(pg_text_config.vocab_size, pg_text_config.hidden_size)\n",
    "text_embedding_layer.weight.data = text_weights['model.embed_tokens.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run dirty Paligemma inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1152])\n"
     ]
    }
   ],
   "source": [
    "image_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "img = processor.image_processor(raw_image).pixel_values[0]\n",
    "img = torch.tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1152])\n"
     ]
    }
   ],
   "source": [
    "# produce the visual embedding\n",
    "with torch.no_grad():\n",
    "    visual_emb = vit(img.unsqueeze(0))\n",
    "\n",
    "print(visual_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 2048])\n"
     ]
    }
   ],
   "source": [
    "# project the visual embedding to the text embedding\n",
    "visual_emb = visual_embedding_to_text_embedding(visual_emb)\n",
    "print(text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 2048])\n"
     ]
    }
   ],
   "source": [
    "# let's emmbed the text\n",
    "prompt = \"What is on the flower?\"\n",
    "tokens = processor.tokenizer(prompt, return_tensors=\"pt\")\n",
    "text_emb = text_embedding_layer(tokens['input_ids'])\n",
    "print(text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 263, 2048])\n"
     ]
    }
   ],
   "source": [
    "# let's concatenate the visual and text embeddings\n",
    "multi_modal_emb = torch.cat((visual_emb, text_emb), dim=1)\n",
    "print(multi_modal_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 95.2791, -16.8974,   0.9396,  ..., -10.6272,   5.6287,  21.7257],\n",
       "         [-41.0327,   1.5128,  27.5928,  ...,  -6.2910,  17.8286,   1.0437],\n",
       "         [ 81.8929, -15.0253,  18.9915,  ..., -15.1030,  26.8091,  -9.5997],\n",
       "         ...,\n",
       "         [ 10.9351,  -1.4130,  -4.7609,  ...,  -0.3168,   2.1305,   1.1936],\n",
       "         [ 13.5130,  -2.6200,   2.3359,  ...,   1.4821,   4.2262,  -1.0913],\n",
       "         [  9.9381,  -1.2802,  -5.5774,  ...,  -1.7720,   0.5118,  -3.4634]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's normalize the multi-modal embedding as in gamma\n",
    "normalizer = torch.tensor(pg_text_config.hidden_size**0.5, dtype=multi_modal_emb.dtype, device=multi_modal_emb.device)\n",
    "multi_modal_emb_norm = multi_modal_emb * normalizer\n",
    "multi_modal_emb_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[164.1517,  44.3966,  45.8237,  ..., 106.8878, 107.1223, 107.1785],\n",
       "         [198.8348,  62.3247,  34.0407,  ..., 132.9104, 133.1945, 133.2679],\n",
       "         [187.5081,  53.3461,  37.0409,  ..., 124.0491, 124.3092, 124.3831],\n",
       "         ...,\n",
       "         [ -4.9033,   4.9402, -19.8332,  ...,  -4.1096,  -4.1100,  -4.1222],\n",
       "         [ -3.5158,   6.9640, -11.5104,  ...,  -3.1971,  -3.2035,  -3.2032],\n",
       "         [ -7.5126,   6.9775, -13.5781,  ...,  -5.2280,  -5.2349,  -5.2321]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's pass the multi-modal embedding through the gemma model\n",
    "gemma_out = gemma(multi_modal_emb_norm)\n",
    "# gemma_out.shape\n",
    "gemma_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = F.softmax(gemma_out[0,-1], dim=-1).argmax()\n",
    "processor.tokenizer.decode(next_token.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
